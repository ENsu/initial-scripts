{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "\n",
    "from initial import parse_company_page, post_process_company_df, parse_round_page, post_process_round_df, parse_investor_page, post_process_investor_df, parse_acquisition_page, post_process_acquisition_df, map_company_status, map_acq_amount_to_num\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import get_base_domain, cleanup_prefectur_info, get_usd, split2row_with_index, json_col_to_df, export_to_s3, validate_datetime_str_format, validate_date_str_format\n",
    "from cb import generate_cb_uuid\n",
    "from datetime import datetime\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = storage.Client()\n",
    "bucket: Bucket = client.get_bucket(\"initial-htmls\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data from GCS\n",
    "## Parse Company Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"initial.inc/companies\")\n",
    "blob_cnt = sum(1 for _ in blobs)\n",
    "\n",
    "company_df_list = []\n",
    "blobs = bucket.list_blobs(prefix=\"initial.inc/companies\")\n",
    "for blob in tqdm(blobs, total=blob_cnt):\n",
    "    html_byte = blob.download_as_string()\n",
    "    df = parse_company_page(html_byte.decode('utf-8'))\n",
    "    df['_timestamp'] = blob.name.split(\"/\")[-1]\n",
    "    company_df_list.append(df)\n",
    "\n",
    "company_df = pd.concat(company_df_list, axis=0)\n",
    "print(len(company_df))\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = post_process_company_df(company_df)\n",
    "print(len(company_df))\n",
    "company_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse funding round page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"initial.inc/rounds\")\n",
    "blob_cnt = sum(1 for _ in blobs)\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=\"initial.inc/rounds\")\n",
    "rounds_df_list = []\n",
    "\n",
    "for blob in tqdm(blobs, total=blob_cnt):\n",
    "    html_byte = blob.download_as_string()\n",
    "    df = parse_round_page(html_byte.decode('utf-8'))\n",
    "    df['_timestamp'] = blob.name.split(\"/\")[-1]\n",
    "    rounds_df_list.append(df)\n",
    "\n",
    "rounds_df = pd.concat(rounds_df_list, axis=0).reset_index(drop=True)\n",
    "print(len(rounds_df))\n",
    "rounds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_df = post_process_round_df(rounds_df)\n",
    "print(len(rounds_df))\n",
    "rounds_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse investor page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"initial.inc/investors\")\n",
    "blob_cnt = sum(1 for _ in blobs)\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=\"initial.inc/investors\")\n",
    "investor_dicts = []\n",
    "\n",
    "for blob in tqdm(blobs, total=blob_cnt):\n",
    "    html_byte = blob.download_as_string()\n",
    "    my_dict = parse_investor_page(html_byte.decode('utf-8'))\n",
    "    investor_dicts.append({**my_dict, \"Company url\": '/' + '/'.join(blob.name.split(\"/\")[-3:-1]), \"_timestamp\": blob.name.split(\"/\")[-1]})\n",
    "\n",
    "investor_df = pd.DataFrame(investor_dicts)\n",
    "investor_df = investor_df[investor_df['Company url'] != '']\n",
    "print(len(investor_df))\n",
    "investor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investor_df = post_process_investor_df(investor_df)\n",
    "investor_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse aquisition page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs(prefix=\"initial.inc/finance_news\")\n",
    "blob_cnt = sum(1 for _ in blobs)\n",
    "\n",
    "blobs = bucket.list_blobs(prefix=\"initial.inc/finance_news\")\n",
    "aquisition_dicts = []\n",
    "html_type = \"\"\n",
    "\n",
    "for blob in tqdm(blobs, total=blob_cnt):\n",
    "    if blob.name == \"initial.inc/finance_news/category=子会社化\":\n",
    "        continue\n",
    "    html_byte = blob.download_as_string()\n",
    "    my_dict = parse_acquisition_page(html_byte.decode('utf-8'))\n",
    "    aquisition_dicts.append({**my_dict, \"_timestamp\": blob.name.split(\"/\")[-1]})\n",
    "    \n",
    "acquisition_df = pd.DataFrame(aquisition_dicts)\n",
    "acquisition_df = acquisition_df[~acquisition_df['acquirer'].isnull()]\n",
    "print(len(acquisition_df))\n",
    "acquisition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_df = split2row_with_index(acquisition_df, 'acquirer', show_index=True)\n",
    "acquisition_df = post_process_acquisition_df(acquisition_df)\n",
    "acquisition_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Transfer data to CB format\n",
    "## Clean up company's prefecture table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df['Prefectures'] = cleanup_prefectur_info(company_df['Prefectures'])\n",
    "company_prefectur_map = company_df.set_index('Company url')['Prefectures']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CB Funding Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_cols = ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
    "       'updated_at', 'country_code', 'state_code', 'region', 'city',\n",
    "       'investment_type', 'announced_on', 'raised_amount_usd', 'raised_amount',\n",
    "       'raised_amount_currency_code', 'post_money_valuation_usd',\n",
    "       'post_money_valuation', 'post_money_valuation_currency_code',\n",
    "       'investor_count', 'org_uuid', 'org_name', 'lead_investor_uuids']\n",
    "\n",
    "for date_col in ['Procurement date']:\n",
    "       rounds_df[date_col] = rounds_df[date_col].apply(lambda x: datetime.strptime(x, \"%Y/%m/%d\").strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "rounds_df['uuid'] = rounds_df['Round Url'].apply(lambda x: generate_cb_uuid(\"funding_rounds\", x))\n",
    "rounds_df['name'] = rounds_df.apply(lambda row: f\"{row['INITIAL series']} - {row['Company Name']}\", axis=1)\n",
    "rounds_df['type'] = 'funding_round'\n",
    "rounds_df.rename(columns={'Round Url': 'permalink'}, inplace=True)\n",
    "rounds_df['cb_url'] = ''\n",
    "rounds_df['rank'] = ''\n",
    "rounds_df['created_at'] = rounds_df['Procurement date']\n",
    "rounds_df['updated_at'] = rounds_df['Procurement date']\n",
    "rounds_df['country_code'] = 'JP'\n",
    "rounds_df['state_code'] = ''\n",
    "rounds_df['region'] = rounds_df['Company Url'].map(company_prefectur_map)\n",
    "rounds_df['city'] = ''\n",
    "rounds_df['investment_type'] = rounds_df['INITIAL series'].apply(lambda x: x.lower().replace(\" \", \"_\"))\n",
    "rounds_df['investment_type'] = rounds_df['investment_type'].apply(lambda x: x if x else 'undisclosed')\n",
    "rounds_df.rename(columns={'Procurement date': 'announced_on'}, inplace=True)\n",
    "\n",
    "rounds_df['raised_amount'] = rounds_df['Funding Amount.(thousand yen)'].apply((lambda x: float(x.replace(\",\", \"\")) * 1e3 if x else 0))\n",
    "rounds_df['raised_amount_currency_code'] = 'JPY'\n",
    "rounds_df['raised_amount_usd'] = rounds_df.apply(lambda row: get_usd(row['raised_amount_currency_code'], row['announced_on'], row['raised_amount']), axis=1)\n",
    "rounds_df['post_money_valuation'] = rounds_df['Pre-Money Valuation.(1,000 yen)'].apply((lambda x: float(x.replace(\",\", \"\")) * 1e3 if x else 0))\n",
    "rounds_df['post_money_valuation_currency_code'] = 'JPY'\n",
    "rounds_df['post_money_valuation_usd'] = rounds_df.apply(lambda row: get_usd(row['post_money_valuation_currency_code'], row['announced_on'], row['post_money_valuation']), axis=1)\n",
    "\n",
    "rounds_df['investor_count'] = rounds_df['Investors'].apply(len)\n",
    "rounds_df['org_uuid'] = rounds_df['Company Url'].apply(lambda x: generate_cb_uuid(\"organization\", x))\n",
    "rounds_df.rename(columns={'Company Name': 'org_name'}, inplace=True)\n",
    "\n",
    "rounds_df['lead_investor_uuids'] = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Funding Round data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_rounds_df = rounds_df[fr_cols].copy()\n",
    "assert (cb_rounds_df['created_at'] == '').sum() == 0, 'missing created_at value'\n",
    "assert (cb_rounds_df['announced_on'] == '').sum() == 0, 'missing announced_on value'\n",
    "assert cb_rounds_df['uuid'].nunique() == len(cb_rounds_df), 'It seems like the uuid is not unique across the table'\n",
    "assert cb_rounds_df['investment_type'].isin(['undisclosed', 'seed', 'series_a', 'series_b', 'series_c', 'series_d', 'series_e', 'series_f', 'series_g']).all(), 'There are some invalid investment_type values'\n",
    "for c in ['created_at', 'updated_at']:\n",
    "    cb_rounds_df.loc[:, c] = cb_rounds_df[c].apply(lambda x: x + \" 00:00:00\")\n",
    "    assert cb_rounds_df[c].apply(validate_datetime_str_format).all(), f'There are some invalid date format in cb_rounds_df[{c}]'\n",
    "for c in ['announced_on']:\n",
    "    assert cb_rounds_df[c].apply(validate_date_str_format).all(), f'There are some invalid date format in cb_rounds_df[{c}]'\n",
    "for c in ['raised_amount', 'raised_amount_usd', 'post_money_valuation', 'post_money_valuation_usd']:\n",
    "    cb_rounds_df[c] = cb_rounds_df[c].fillna(0).astype(int)\n",
    "    assert cb_rounds_df[c].apply(lambda x: x >= 0).all(), f'There are some invalid value in cb_rounds_df[{c}]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get investments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investments_df = json_col_to_df(split2row_with_index(rounds_df[['Investors', 'name', 'permalink', 'uuid', 'announced_on']], 'Investors', show_index=True).reset_index(drop=True), col_name='Investors', prefix=True)\n",
    "investments_df = investments_df.drop_duplicates(['permalink', 'Investors_url', 'announced_on'])\n",
    "investments_df['Investors_url'].fillna('', inplace=True)\n",
    "investments_df['Investors_name'].fillna('', inplace=True)\n",
    "investments_df = investments_df[~(investments_df['Investors_url'] == '')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get investors urls for crawling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investors_to_crawl = investments_df.loc[~investments_df['Investors_url'].isin(investor_df['Company url']), ['Investors_name', 'Investors_url']].drop_duplicates()\n",
    "investors_to_crawl.columns = ['name', 'url']\n",
    "acquirers_to_crawl = acquisition_df.loc[~acquisition_df['Acquirer Url'].isin(investor_df['Company url']), ['Acquirer Name', 'Acquirer Url']].drop_duplicates()\n",
    "acquirers_to_crawl.columns = ['name', 'url']\n",
    "pd.concat([investors_to_crawl, acquirers_to_crawl], axis=0).drop_duplicates().to_csv('investors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cols = ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
    "       'updated_at', 'funding_round_uuid', 'funding_round_name',\n",
    "       'investor_uuid', 'investor_name', 'investor_type', 'is_lead_investor']\n",
    "\n",
    "\n",
    "investments_df.rename(columns={'uuid': 'funding_round_uuid'}, inplace=True)\n",
    "investments_df.rename(columns={'name': 'funding_round_name'}, inplace=True)\n",
    "investments_df['uuid'] = investments_df.apply(lambda row: generate_cb_uuid(\"investments\", row['permalink'] + row['Investors_url']), axis=1)\n",
    "\n",
    "investments_df['name'] = investments_df.apply(lambda row: f\"{row['Investors_name']} in {row['funding_round_name']}\", axis=1)\n",
    "investments_df['type'] = 'investment'\n",
    "investments_df['permalink'] = ''\n",
    "investments_df['cb_url'] = ''\n",
    "investments_df['rank'] = ''\n",
    "investments_df['created_at'] = investments_df['announced_on']\n",
    "investments_df['updated_at'] = investments_df['announced_on']\n",
    "investments_df['investor_uuid'] = investments_df['Investors_url'].apply(lambda x: generate_cb_uuid(\"organization\", x))\n",
    "investments_df.rename(columns={'Investors_name': 'investor_name'}, inplace=True)\n",
    "investments_df['investor_type'] = 'organization'\n",
    "investments_df['is_lead_investor'] = False\n",
    "\n",
    "cb_investments_df = investments_df[inv_cols].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Investment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert investments_df['Investors_url'].isin(investor_df['Company url']).all()\n",
    "assert (cb_investments_df['created_at'] == '').sum() == 0, 'missing created_at value'\n",
    "assert cb_investments_df['uuid'].nunique() == len(cb_investments_df), 'It seems like the uuid is not unique across the table'\n",
    "assert cb_investments_df['funding_round_uuid'].isin(cb_rounds_df['uuid']).all(), 'There are some invalid funding_round_uuid values'\n",
    "for c in ['created_at', 'updated_at']:\n",
    "    cb_investments_df.loc[:, c] = cb_investments_df[c].apply(lambda x: x + \" 00:00:00\").copy()\n",
    "    assert cb_investments_df[c].apply(validate_datetime_str_format).all(), f'There are some invalid date format in cb_investments_df[{c}]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Organization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date_col in ['Total Procurement Calculation Date', 'Valuation calculation round implementation date', 'Founded Date', 'Last Funding Date', 'registration date', 'update date', 'IPO date']:\n",
    "       company_df[date_col] = company_df[date_col].apply(lambda x: datetime.strptime(x, \"%Y/%m/%d\").strftime(\"%Y-%m-%d\") if x else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine organization_df and investor_df\n",
    "company_df['roles'] = 'company'\n",
    "company_df['primary_role'] = 'company'\n",
    "company_df['website'].fillna(\"\", inplace=True)\n",
    "print(f\"Got total of {len(company_df)} companies, dropping {(company_df['website'] == '').sum()} companies without website\")\n",
    "company_df = company_df[company_df['website'] != '']\n",
    "\n",
    "investor_df['roles'] = 'investor'\n",
    "investor_df['primary_role'] = 'investor'\n",
    "investor_df['website'].fillna(\"\", inplace=True)\n",
    "investor_df['registration date'] = '1970-01-01'\n",
    "investor_df['update date'] = datetime.today().date().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Got total of {len(investor_df)} investors, droping {(investor_df['website'] == '').sum()} investors without website\")\n",
    "investor_df = investor_df[investor_df['website'] != '']\n",
    "\n",
    "organization_df = pd.concat([company_df, investor_df], ignore_index=True, axis=0)\n",
    "assert len(organization_df.columns) == len(company_df.columns), \"concat company_df and investor_df shouldn't increate the number of columns\"\n",
    "print(f\"Total remaining organizations {len(organization_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_cols = ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
    "       'updated_at', 'legal_name', 'roles', 'domain', 'homepage_url',\n",
    "       'country_code', 'state_code', 'region', 'city', 'address',\n",
    "       'postal_code', 'status', 'short_description', 'category_list',\n",
    "       'category_groups_list', 'num_funding_rounds', 'total_funding_usd',\n",
    "       'total_funding', 'total_funding_currency_code', 'founded_on',\n",
    "       'last_funding_on', 'closed_on', 'employee_count', 'email', 'phone',\n",
    "       'facebook_url', 'linkedin_url', 'twitter_url', 'logo_url', 'alias1',\n",
    "       'alias2', 'alias3', 'primary_role', 'num_exits']\n",
    "\n",
    "organization_df['uuid'] = organization_df['Company url'].apply(lambda x: generate_cb_uuid(\"organization\", x))\n",
    "organization_df.rename(columns={'Company name': 'name'}, inplace=True)\n",
    "organization_df['type'] = \"organization\"\n",
    "organization_df.rename(columns={'Company url': 'permalink'}, inplace=True)\n",
    "organization_df['cb_url'] = ''\n",
    "organization_df['rank'] = ''\n",
    "organization_df.rename(columns={'registration date': 'created_at'}, inplace=True)\n",
    "organization_df.rename(columns={'update date': 'updated_at'}, inplace=True)\n",
    "\n",
    "organization_df['legal_name'] = ''\n",
    "# organization_df['roles'] Done before concat\n",
    "organization_df['domain'] = organization_df['website'].apply(get_base_domain).fillna('')\n",
    "organization_df.rename(columns={'website': 'homepage_url'}, inplace=True)\n",
    "organization_df['country_code'] = \"JP\"\n",
    "organization_df['state_code'] = ''\n",
    "organization_df.rename(columns={'Prefectures': 'region'}, inplace=True)\n",
    "organization_df['city'] = ''\n",
    "# organization_df['address'] = organization_df['address']\n",
    "organization_df['postal_code'] = ''\n",
    "organization_df['status'] = organization_df['Investigation status'].map(map_company_status)\n",
    "\n",
    "organization_df.rename(columns={'Description': 'short_description'}, inplace=True)\n",
    "organization_df['category_list'] = [[]] * len(organization_df) # TODO. ask Conrad and Kalle to see if these twos are necessary, if necessary, generated by tags and industry\n",
    "organization_df['category_groups_list'] = [[]] * len(organization_df)\n",
    "organization_df['num_funding_rounds'] = organization_df['uuid'].map(cb_rounds_df.groupby('org_uuid').size())\n",
    "organization_df['total_funding'] = organization_df['uuid'].map(cb_rounds_df.groupby('org_uuid')['raised_amount'].sum())\n",
    "organization_df['total_funding_currency_code'] = 'JPY'\n",
    "organization_df['total_funding_usd'] = organization_df['uuid'].map(cb_rounds_df.groupby('org_uuid')['raised_amount_usd'].sum())\n",
    "organization_df.rename(columns={'Founded Date': 'founded_on'}, inplace=True)\n",
    "organization_df['founded_on'].fillna('', inplace=True)\n",
    "organization_df.rename(columns={'Last Funding Date': 'last_funding_on'}, inplace=True)\n",
    "organization_df['last_funding_on'].fillna('', inplace=True)\n",
    "organization_df['closed_on'] = ''\n",
    "organization_df.rename(columns={'number of employees': 'employee_count'}, inplace=True)\n",
    "organization_df['email'] = ''\n",
    "organization_df.rename(columns={'phone number': 'phone'}, inplace=True)\n",
    "organization_df['facebook_url'] = ''\n",
    "organization_df['linkedin_url'] = ''\n",
    "organization_df['twitter_url'] = ''\n",
    "organization_df['logo_url'] = ''\n",
    "organization_df['alias1'] = ''\n",
    "organization_df['alias2'] = ''\n",
    "organization_df['alias3'] = ''\n",
    "# organization_df['primary_role'] Done before concat\n",
    "organization_df['num_exits'] = ''\n",
    "\n",
    "cb_organization_df = organization_df[org_cols].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Organization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cb_organization_df['uuid'].nunique() == len(cb_organization_df), 'It seems like the uuid is not unique across the table'\n",
    "assert (cb_organization_df['created_at'] == '').sum() == 0, 'missing created_at value'\n",
    "\n",
    "# Cleanup data without domain\n",
    "assert (cb_organization_df['domain']=='').sum() == 0, 'cb_organization_df has companies who missed domain value'\n",
    "for c in ['created_at', 'updated_at']:\n",
    "    cb_organization_df.loc[:, c] = cb_organization_df[c].apply(lambda x: x + \" 00:00:00\").copy()\n",
    "    assert cb_organization_df[c].apply(validate_datetime_str_format).all(), f'There are some invalid date format in cb_organization_df[{c}]'\n",
    "\n",
    "for c in ['founded_on', 'last_funding_on', 'closed_on']:\n",
    "    non_blank_cond = cb_organization_df[c] != ''\n",
    "    cb_organization_df.loc[non_blank_cond, c] = cb_organization_df.loc[non_blank_cond, c].apply(lambda x: x + \" 00:00:00\").copy()\n",
    "    assert cb_organization_df[c].apply(validate_datetime_str_format, blankable=True).all(), f'There are some invalid date format in cb_organization_df[{c}]'\n",
    "\n",
    "for c in ['total_funding', 'total_funding_usd', 'num_funding_rounds']:\n",
    "    cb_organization_df[c] = cb_organization_df[c].fillna(0).astype(int)\n",
    "    assert cb_organization_df[c].apply(lambda x: x >= 0).all(), f'There are some invalid value in cb_organization_df[{c}]'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get IPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipo_cols = ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
    "       'updated_at', 'org_uuid', 'org_name', 'org_cb_url', 'country_code',\n",
    "       'state_code', 'region', 'city', 'stock_exchange_symbol', 'stock_symbol',\n",
    "       'went_public_on', 'share_price_usd', 'share_price',\n",
    "       'share_price_currency_code', 'valuation_price_usd', 'valuation_price',\n",
    "       'valuation_price_currency_code', 'money_raised_usd', 'money_raised',\n",
    "       'money_raised_currency_code']\n",
    "\n",
    "ipo_df = organization_df[organization_df['status'] == 'ipo'].copy()\n",
    "ipo_df.rename(columns={'uuid': 'org_uuid'}, inplace=True)\n",
    "ipo_df.rename(columns={'name': 'org_name'}, inplace=True)\n",
    "ipo_df.rename(columns={'cb_url': 'org_cb_url'}, inplace=True)\n",
    "\n",
    "ipo_df['uuid'] = ipo_df.apply(lambda row: generate_cb_uuid(\"ipo\", row['permalink'] + row['IPO date']), axis=1)\n",
    "ipo_df['name'] = ''\n",
    "ipo_df['type'] = 'ipo'\n",
    "ipo_df['permalink'] = ''\n",
    "ipo_df['cb_url'] = ''\n",
    "ipo_df['rank'] = ''\n",
    "ipo_df.rename(columns={'registration date': 'created_at'}, inplace=True)\n",
    "ipo_df.rename(columns={'update date': 'updated_at'}, inplace=True)\n",
    "# country_code, state_code, region, city already processed and exists\n",
    "\n",
    "ipo_df['stock_exchange_symbol'] = ''\n",
    "ipo_df['stock_symbol'] = ''\n",
    "ipo_df.rename(columns={'IPO date': 'went_public_on'}, inplace=True)\n",
    "ipo_df['share_price_usd'] = ''\n",
    "ipo_df['share_price'] = ''\n",
    "ipo_df['share_price_currency_code'] = 'JPY'\n",
    "ipo_df['valuation_price'] = ipo_df['Market capitalization at the time of IPO (initial price).(One million yen)'].apply((lambda x: float(x.replace(\",\", \"\")) * 1e6 if x else 0))\n",
    "ipo_df['valuation_price_currency_code'] = 'JPY'\n",
    "ipo_df['valuation_price_usd'] = ipo_df.apply(lambda row: get_usd(row['valuation_price_currency_code'], row['went_public_on'], row['valuation_price']), axis=1)\n",
    "ipo_df['money_raised_usd'] = ''\n",
    "ipo_df['money_raised'] = ''\n",
    "ipo_df['money_raised_currency_code'] = 'JPY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test IPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_ipo_df = ipo_df[ipo_cols].copy()\n",
    "\n",
    "for c in ['created_at', 'updated_at']:\n",
    "    cb_ipo_df.loc[:, c] = cb_ipo_df[c].apply(lambda x: x + \" 00:00:00\").copy()\n",
    "    assert cb_ipo_df[c].apply(validate_datetime_str_format).all(), f'There are some invalid date format in cb_ipo_df[{c}]'\n",
    "\n",
    "for c in ['went_public_on']:\n",
    "    assert cb_ipo_df[c].apply(validate_date_str_format, blankable=True).all(), f'There are some invalid date format in cb_ipo_df[{c}]'\n",
    "    \n",
    "for c in ['valuation_price', 'valuation_price_usd']:\n",
    "    cb_ipo_df[c] = cb_ipo_df[c].fillna(0).astype(int)\n",
    "    assert cb_ipo_df[c].apply(lambda x: x >= 0).all(), f'There are some invalid value in cb_ipo_df[{c}]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Acquisition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_cols = ['uuid', 'name', 'type', 'permalink', 'cb_url', 'rank', 'created_at',\n",
    "       'updated_at', 'acquiree_uuid', 'acquiree_name', 'acquiree_cb_url',\n",
    "       'acquiree_country_code', 'acquiree_state_code', 'acquiree_region',\n",
    "       'acquiree_city', 'acquirer_uuid', 'acquirer_name', 'acquirer_cb_url',\n",
    "       'acquirer_country_code', 'acquirer_state_code', 'acquirer_region',\n",
    "       'acquirer_city', 'acquisition_type', 'acquired_on', 'price_usd',\n",
    "       'price', 'price_currency_code']\n",
    "\n",
    "acquisition_df['Date'] = acquisition_df['Date'].apply(lambda x: datetime.strptime(x, \"%Y/%m/%d\").strftime(\"%Y-%m-%d\") if x else '')\n",
    "\n",
    "acquisition_df['uuid'] = acquisition_df.apply(lambda row: generate_cb_uuid(\"acquisition\", row['Startup Url'] + row['Date'] + row['Acquirer Url']), axis=1)\n",
    "acquisition_df['name'] = acquisition_df.apply(lambda row: f\"{row['Startup Name']} acquired by {row['Acquirer Name']}\", axis=1)\n",
    "acquisition_df['type'] = 'acquisition'\n",
    "acquisition_df['permalink'] = ''\n",
    "acquisition_df['cb_url'] = ''\n",
    "acquisition_df['rank'] = ''\n",
    "acquisition_df['created_at'] = acquisition_df['Date']\n",
    "acquisition_df['updated_at'] = acquisition_df['Date']\n",
    "acquisition_df['acquiree_uuid'] = acquisition_df['Startup Url'].apply(lambda x: generate_cb_uuid(\"organization\", x))\n",
    "acquisition_df.rename(columns={'Startup Name': 'acquiree_name'}, inplace=True)\n",
    "acquisition_df['acquiree_cb_url'] = ''\n",
    "acquisition_df['acquiree_country_code'] = 'JP'\n",
    "acquisition_df['acquiree_state_code'] = ''\n",
    "acquisition_df['acquiree_region'] = acquisition_df['acquiree_uuid'].map(organization_df.set_index('uuid')['region'])\n",
    "acquisition_df['acquiree_city'] = ''\n",
    "\n",
    "acquisition_df['acquirer_uuid'] = acquisition_df['Acquirer Url'].apply(lambda x: generate_cb_uuid(\"organization\", x))\n",
    "acquisition_df.rename(columns={'Acquirer Name': 'acquirer_name'}, inplace=True)\n",
    "acquisition_df['acquirer_cb_url'] = ''\n",
    "acquisition_df['acquirer_country_code'] = 'JP'\n",
    "acquisition_df['acquirer_state_code'] = ''\n",
    "acquisition_df['acquirer_region'] = acquisition_df['acquirer_uuid'].map(organization_df.set_index('uuid')['region'])\n",
    "acquisition_df['acquirer_city'] = ''\n",
    "\n",
    "acquisition_df['acquisition_type'] = 'acquisition'\n",
    "acquisition_df.rename(columns={'Date': 'acquired_on'}, inplace=True)\n",
    "\n",
    "acquisition_df['price_currency_code'] = 'JPY'\n",
    "acquisition_df['price'] = acquisition_df['Acquisition amount'].apply(map_acq_amount_to_num)\n",
    "acquisition_df['price_usd'] = acquisition_df.apply(lambda row: get_usd(row['price_currency_code'], row['acquired_on'], row['price']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_acquisition_df = acquisition_df[acq_cols].copy()\n",
    "\n",
    "for c in ['created_at', 'updated_at']:\n",
    "    cb_acquisition_df.loc[:, c] = cb_acquisition_df[c].apply(lambda x: x + \" 00:00:00\").copy()\n",
    "    assert cb_acquisition_df[c].apply(validate_datetime_str_format).all(), f'There are some invalid date format in cb_acquisition_df[{c}]'\n",
    "    \n",
    "for c in ['price_usd', 'price']:\n",
    "    cb_acquisition_df[c] = cb_acquisition_df[c].fillna(0).astype(int)\n",
    "    assert cb_acquisition_df[c].apply(lambda x: x >= 0).all(), f'There are some invalid value in cb_acquisition_df[{c}]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data is properly joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_uuids = cb_organization_df['uuid']\n",
    "assert cb_organization_df.columns.tolist() == org_cols, \"cb_organization_df columns are not the same as org_cols\"\n",
    "\n",
    "print(\"Remove rows in rounds_df where its org_uuid is not in org_uuids\")\n",
    "print(f\"Remove {len(cb_rounds_df) - cb_rounds_df['org_uuid'].isin(org_uuids).sum()} rows of funding rounds data...\")\n",
    "cb_rounds_df = cb_rounds_df[cb_rounds_df['org_uuid'].isin(org_uuids)]\n",
    "assert cb_rounds_df['org_uuid'].isin(org_uuids).all(), 'There are org_uuids in rounds_df that are not in org_uuids'\n",
    "print(f\"{len(cb_rounds_df)} rows of funding rounds data remain\")\n",
    "assert cb_rounds_df.columns.tolist() == fr_cols, \"cb_rounds_df columns are not the same as fr_cols\"\n",
    "\n",
    "print(\"Remove rows in investments_df where its org_uuid is not in org_uuids\")\n",
    "print(f\"Remove {len(cb_investments_df) - cb_investments_df['investor_uuid'].isin(org_uuids).sum()} rows of investments data...\")\n",
    "cb_investments_df = cb_investments_df[cb_investments_df['investor_uuid'].isin(org_uuids)]\n",
    "assert cb_investments_df['investor_uuid'].isin(org_uuids).all(), 'There are investor_uuid in investments_df that are not in org_uuids'\n",
    "print(f\"{len(cb_investments_df)} rows of investment data remain\")\n",
    "assert cb_investments_df.columns.tolist() == inv_cols, \"cb_investments_df columns are not the same as inv_cols\"\n",
    "\n",
    "\n",
    "assert cb_ipo_df['org_uuid'].isin(org_uuids).all(), 'There are org_uuids in ipo_df that are not in org_uuids'\n",
    "assert cb_ipo_df.columns.tolist() == ipo_cols, \"cb_ipo_df columns are not the same as ipo_cols\"\n",
    "\n",
    "# Check acquiree_uuid\n",
    "print(\"Remove rows in acquisition_df where its acquiree_uuid is not in org_uuids\")\n",
    "print(f\"Remove {len(cb_acquisition_df) - cb_acquisition_df['acquiree_uuid'].isin(org_uuids).sum()} rows of acquisition data...\")\n",
    "cb_acquisition_df = cb_acquisition_df[cb_acquisition_df['acquiree_uuid'].isin(org_uuids)]\n",
    "assert cb_acquisition_df['acquiree_uuid'].isin(org_uuids).all(), 'There are acquiree_uuid in cb_acquisition_df that are not in org_uuids'\n",
    "print(f\"{len(cb_acquisition_df)} rows of acquisition data remain\")\n",
    "\n",
    "\n",
    "# Check acquirer_uuid\n",
    "print(\"Remove rows in acquisition_df where its acquirer_uuid is not in org_uuids\")\n",
    "print(f\"Remove {len(cb_acquisition_df) - cb_acquisition_df['acquirer_uuid'].isin(org_uuids).sum()} rows of acquisition data...\")\n",
    "cb_acquisition_df = cb_acquisition_df[cb_acquisition_df['acquirer_uuid'].isin(org_uuids)]\n",
    "assert cb_acquisition_df['acquirer_uuid'].isin(org_uuids).all(), 'There are acquirer_uuid in cb_acquisition_df that are not in org_uuids'\n",
    "print(f\"{len(cb_acquisition_df)} rows of acquisition data remain\")\n",
    "assert cb_acquisition_df.columns.tolist() == acq_cols, \"cb_acquisition_df columns are not the same as acq_cols\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log table size for traction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.add('initial_to_eva.log')\n",
    "logger.info(f\"time: {datetime.now()}: cb_organization_df shape: {cb_organization_df.shape}\")\n",
    "logger.info(f\"time: {datetime.now()}: cb_rounds_df shape: {cb_rounds_df.shape}\")\n",
    "logger.info(f\"time: {datetime.now()}: cb_investments_df shape: {cb_investments_df.shape}\")\n",
    "logger.info(f\"time: {datetime.now()}: cb_ipo_df shape: {cb_ipo_df.shape}\")\n",
    "logger.info(f\"time: {datetime.now()}: cb_acquisition_df shape: {cb_acquisition_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_to_s3...\n",
    "export_to_s3(cb_organization_df, \"organizations.csv\")\n",
    "export_to_s3(cb_rounds_df, \"funding_rounds.csv\")\n",
    "export_to_s3(cb_investments_df, 'investments.csv')\n",
    "export_to_s3(cb_ipo_df, 'ipos.csv')\n",
    "export_to_s3(cb_acquisition_df, 'acquisitions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
